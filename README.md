# ğŸ¤– Agentic Benchmarks

> Pitting AI models against each other in real-world coding challenges.

This repository hosts a collection of benchmarks designed to evaluate how well different AI models perform on practical programming tasks.

## ğŸ“š Benchmarks

| Benchmark | Category | Description |
|-----------|----------|-------------|
| [1 Billion Row Challenge](./1brc/) | Performance | Process 1B temperature readings as fast as possible |
| [Project Euler](./projecteuler/) | Reasoning/Algorithm | Solve mathematical and programming problems |

## ğŸï¸ 1BRC at a Glance

| Rank | Implementation | Time |
|------|----------------|------|
| ğŸ¥‡ | go-gemini3-with-hint | 91.5ms |
| ğŸ¥ˆ | go-opus4.5-with-hint | 146.2ms |
| ğŸ¥‰ | go-opus4.5 | 174.7ms |
| 4 | go-haiku-4.5 | 195.1ms |
| 5 | go-gemini3 | 220.6ms |

[View full results â†’](./1brc/)

## ğŸ“‚ Repository Structure

```
agentic-benchmarks/
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ 1brc/               # 1 Billion Row Challenge
â”œâ”€â”€ projecteuler/       # Project Euler Challenge
â””â”€â”€ ...
```

Each benchmark has its own directory with setup instructions, prompts, implementations, and results.
